{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningAssignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIF97aXw8ZOT",
        "colab_type": "text"
      },
      "source": [
        "## **Predicting Video Memorability Score**\n",
        "\n",
        "This notebook contains multiple approaches to predicint memorability scores. The features provded by MediaEval were transformed in a number of ways which are outlined in the Pre-Processing section. In order to determine the optimum feature and model combination, each feature was trained on each model. After Pre-Processing, every section contains a Machine Learning algorithm and each subsection contains models trained on the different features. The ML algorithms implemented are: \n",
        "\n",
        "\n",
        "\n",
        "*   Linear Regression\n",
        "*   Ridge Regression\n",
        "*   Decision Tree\n",
        "*   Support Vector Machine\n",
        "*   Random Forest\n",
        "*   Voting Regressor\n",
        "*   Bagging Regressor\n",
        "*   Stacking Regressor\n",
        "\n",
        "I have commented the code, however to avoid being repetitive and overfilling the notebook I do not repeat any comments which explain similar code.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT6iLtSB4ygT",
        "colab_type": "text"
      },
      "source": [
        "# Spearmans Correlation Coefficient Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO1etw_q0h7J",
        "colab_type": "text"
      },
      "source": [
        "The performance of each model will be based on the Spearman correlation score of the predictions and the ground truth. This function was provided by Eoin Brophy and can be found here:\n",
        "\n",
        "https://drive.google.com/drive/folders/1puG9lLjao1y4ZngKHJFpxi4Yl-9cHvV7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ibTueg42sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Get_score(Y_true, Y_pred):\n",
        "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
        "    Y_pred = np.squeeze(Y_pred)\n",
        "    Y_true = np.squeeze(Y_true)\n",
        "    if Y_pred.shape != Y_true.shape:\n",
        "        print('Input shapes don\\'t match!')\n",
        "    else:\n",
        "        if len(Y_pred.shape) == 1:\n",
        "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
        "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
        "            print('The Spearman\\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])\n",
        "        else:\n",
        "            for ii in range(Y_pred.shape[1]):\n",
        "                Get_score(Y_pred[:,ii],Y_true[:,ii])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XoS18YenGGQ",
        "colab_type": "text"
      },
      "source": [
        "# Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6pOGDoIzvLq",
        "colab_type": "text"
      },
      "source": [
        "The initial data acquition step can be found in the file 'LoadDataSaveAsNumpy.ipynb'. In that file the data was loaded and saved as NPY files. In the section 'Load Numpy Files' below I load these files.\n",
        "\n",
        "The features used in this study are:\n",
        "\n",
        "*   Captions\n",
        "*   HMP\n",
        "*   C3D\n",
        "*   Labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tloP51xCecgv",
        "colab_type": "text"
      },
      "source": [
        "## Mapping Drive and Load Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMV-Fwna1MUb",
        "colab_type": "text"
      },
      "source": [
        "I mounted my drive as I was developing this notebook on Google Colabs. \n",
        "\n",
        "I imported all relevant packages for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGyDQ86ObDUF",
        "colab_type": "code",
        "outputId": "01adc594-ad41-4c27-c7cd-86684e91c27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir('/content/drive/My Drive/CA684_Assignment/')\n",
        "\n",
        "#Import Packages\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#Random Seed Set To One\n",
        "from numpy.random import seed\n",
        "seed(1)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thzD2H0Vop-q",
        "colab_type": "text"
      },
      "source": [
        "## Load Numpy Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBCjUGQ41b32",
        "colab_type": "text"
      },
      "source": [
        "As mentioned previously, the inital data acquisition has been carried out in the notebook 'LoadDataSaveAsNumpy.ipynb'. Here the features and labels are loaded from the NPY files created earlier. Doing this allowed the data to be loaded much quicker than it had been loaded from the original dataset files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdjPNJJ0os4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The column names of the labels and captions as these were lost converting the files to npy.\n",
        "captions_column_names = ['video','caption']\n",
        "labels_column_names = ['video','short-term_memorability','nb_short-term_annotations','long-term_memorability','nb_long-term_annotations']\n",
        "\n",
        "#Loading the features from numpy files.\n",
        "loaded_caps = load('/content/drive/My Drive/Assignment/Dataset/captions.npy', allow_pickle=True)\n",
        "loaded_labels = load('/content/drive/My Drive/Assignment/Dataset/labels.npy', allow_pickle=True)\n",
        "loaded_cd3 = load('/content/drive/My Drive/Assignment/Dataset/c3d.npy', allow_pickle=True)\n",
        "loaded_hmp = load('/content/drive/My Drive/Assignment/Dataset/hmp.npy', allow_pickle=True)\n",
        "\n",
        "#Convert Files To Dataframes so they are easier to work with later on\n",
        "df_cap = pd.DataFrame(data=loaded_caps, columns=captions_column_names)\n",
        "df_labels = pd.DataFrame(data=loaded_labels, columns=labels_column_names)\n",
        "df_c3d =  pd.DataFrame(data=loaded_cd3) \n",
        "df_hmp =  pd.DataFrame(data=loaded_hmp) \n",
        "\n",
        "#Correcting the datatypes as these were lost after converting to numpy.\n",
        "df_cap = df_cap.infer_objects()\n",
        "df_labels = df_labels.infer_objects()\n",
        "df_c3d = df_c3d.infer_objects()\n",
        "df_hmp = df_hmp.infer_objects()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV1_hbQUNEaK",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXxcnL9R_NSV",
        "colab_type": "text"
      },
      "source": [
        "The dataset provided consisted of numerical and text features. The text features were the captions and the numberical features consisted of both HMP and C3d. \n",
        "\n",
        "The Machine Learning algorithms used in this study are from Sklearn which work optimally with scaled data. Otherwise, the results from these algorithms could be affected. Therefore, each feature was scaled accoridingly and respective to the type of data.\n",
        "\n",
        "It is also recommended to convert text to numbers when working with machine learning algorithms. Therefore, the text feature Captions was transformed. This was carried out in three different ways so the results could be compared later on in the study. Captions was transformed with Sequences, One Hot Encoding and TF-IDF. As well as this, the standard text cleaning was applied which involved removing stop words and punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ9CubsTARIl",
        "colab_type": "text"
      },
      "source": [
        "## Captions Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P52eKVaa3e6l",
        "colab_type": "text"
      },
      "source": [
        "In other studies of this dataset, captions have been found to perform well for predictions. Therefore, I wanted to get as much use out of the captions as possible. Therefore, I implemented three transformation approaches. Each approach is tested with each machine learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAa4n0T8AkBA",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTP2K1ozAcS4",
        "colab_type": "text"
      },
      "source": [
        "Currently the captions text looks like the results of the first box below. It would be more useful if I split this text into separate words.\n",
        "A simple way to do this is to use a Counter object. Using Counter to create the vocabulary I will use for predictions. I  am updating our dataframe df_cap with the cleaned text because I do not need to keep the previous text.\n",
        "Each word is split and saved in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VQtErFPeVK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "23ee4b84-0c9f-47f0-aaef-997ddd419224"
      },
      "source": [
        "#Looking at the first few rows of our captions we can see they require a lot of cleaning\n",
        "df_cap['caption'].head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   blonde-woman-is-massaged-tilt-down\n",
              "1    roulette-table-spinning-with-ball-in-closeup-shot\n",
              "2                                        khr-gangsters\n",
              "3                 medical-helicopter-hovers-at-airport\n",
              "4                 couple-relaxing-on-picnic-crane-shot\n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bN3qWs7ATFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "45976cbd-7760-4f04-e1ac-ddf4e3ebc9ee"
      },
      "source": [
        "#Setup our Counter object which can assist with cleaning the captions\n",
        "vocab = Counter()\n",
        "\n",
        "#Loop through each caption and clean\n",
        "for i, capitalLetter in enumerate(df_cap['caption']):\n",
        "    # Removing dashes in between words and convert words to lowercase.\n",
        "    text = ''.join([c if c not in punctuation else ' ' for c in capitalLetter]).lower()\n",
        "    #At each row of iteration i save the updated text\n",
        "    df_cap.loc[i,'caption'] = text\n",
        "    vocab.update(text.split())\n",
        "    \n",
        "df_cap['caption'].head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   blonde woman is massaged tilt down\n",
              "1    roulette table spinning with ball in closeup shot\n",
              "2                                        khr gangsters\n",
              "3                 medical helicopter hovers at airport\n",
              "4                 couple relaxing on picnic crane shot\n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRjFuvzhvzcU",
        "colab_type": "text"
      },
      "source": [
        "### One-hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRSFo0txwXpA",
        "colab_type": "text"
      },
      "source": [
        "For machine learning algorithms, it is advised to work with numbers over text. This cell consists of the first approach to transform the data using a technique called ‘One Hot Encoding’.  This simply involves converting the categorical variables into a more machine learning friendly format. Each word will get its own column and so this is going to increase the feature size dramatically, however the benefits of numerical data over categorical are more important here. The main goal of this study is to find an optimum model. \n",
        "\n",
        "First, I used the Tokenizer object and fit it to the captions to encode the data. This allowed me to easily apply one hot encoding.\n",
        "\n",
        "The next step is feature scaling. A test was carried out later down the line with Linear regression and R squared to determine whether this data is linear or non-linear. If the data was linear, I would need to standardise it. It was found to be non-linear, so this requires normalisation, so scaling the data.  I normalised the data to L2 form so all the rows have a unit norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_narA6tDvNry",
        "colab_type": "code",
        "outputId": "63c5f358-0288-4383-ea6d-a2249dd3b3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Setting up the tokenizer\n",
        "num_words = len(vocab)\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "captions_list = list(df_cap.caption.values)\n",
        "\n",
        "#Fitting our Tokenizer on the updated captions\n",
        "tokenizer.fit_on_texts(captions_list) \n",
        "\n",
        "#One-Hot Encoding\n",
        "one_hot_res = tokenizer.texts_to_matrix(list(df_cap.caption.values),mode='binary')\n",
        "\n",
        "#Normalising One-Hot Encoding to l2\n",
        "oh_normalized = normalize(one_hot_res, norm='l2')\n",
        "\n",
        "#Apply PCA for dimensionality reduction while retaining 95% variance.\n",
        "pca = PCA(n_components = 0.95)\n",
        "ohn = pca.fit_transform(oh_normalized)\n",
        "\n",
        "print( \"Shape Of one_hot_res before Normalise and PCA : \" , one_hot_res.shape)\n",
        "print( \"Shape Of one_hot_res after Normalise and PCA : \" , ohn.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of one_hot_res before Normalise and PCA :  (6000, 5191)\n",
            "Shape Of one_hot_res after Normalise and PCA :  (6000, 1778)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTiNYtSi1v6l",
        "colab_type": "text"
      },
      "source": [
        "### Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL4oo7iZ7DAb",
        "colab_type": "text"
      },
      "source": [
        "Using the Tokenizer, I then converted the Captions text into a sequence of integers. Using 'texts_to_sequences'. I then normalise the sequenced data so it is within a range of 0 and 1 and padded them to a length of 50 with zeros. This will ensure the data is in an appropriate representation for the machine learning algorithms used later on.\n",
        "\n",
        "We do not need to reduce dimensionality here because X_seq has only 50 features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITw8H9Iq3S0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "92ac9856-42a4-4473-9b22-f314a13131bf"
      },
      "source": [
        "#Sequence Encoding\n",
        "sequences = tokenizer.texts_to_sequences(list(df_cap.caption.values))\n",
        "\n",
        "#Defining the length I want the sequences all to be\n",
        "max_len = 50\n",
        "\n",
        "#Creating empty numpy array full of zeros to the size of 50\n",
        "X_seq = np.zeros((len(sequences),max_len))\n",
        "#Loop through for the entire contents of sequences\n",
        "for i in range(len(sequences)):\n",
        "    #n is a placeholder which stores the length of seqeunce at the current iteraiton of i\n",
        "    n = len(sequences[i])\n",
        "    #If empty do nothing\n",
        "    if n==0:\n",
        "        print(i)\n",
        "    else:\n",
        "        X_seq[i,-n:] = sequences[i];\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "#Normalise the Sequences\n",
        "seq_normalized = normalize(X_seq, norm='l2')\n",
        "\n",
        "print( \"Shape Of sequences after Normalise and PCA : \" , seq_normalized.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of sequences after Normalise and PCA :  (6000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFtCS3SjmJ-h",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD3ZaDBunwOb",
        "colab_type": "text"
      },
      "source": [
        "To implement some feature engineering, I implemented a pipeline. The pipeline builds a dictionary of features with CountVectorizer and then transforms the features with TF-IDF. CountVectorizer takes care of the standard text pre-processing like filtering of stop words. They were then normalised and the dimensionality reduced. As you can see the number of features reduces by almost 3000 after applying PCA, all while retaining 95% variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWQNeOamOBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "71920325-813d-47e7-c0c6-dc42ee156764"
      },
      "source": [
        "#Create our TF-IDF pipeline\n",
        "tfidf_pipe = Pipeline( [ \n",
        "                       ('count_vec', CountVectorizer()),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                      ])\n",
        "caps = list(df_cap.caption.values)\n",
        "tfidf = tfidf_pipe.fit_transform(caps)\n",
        "\n",
        "#Normalising TFIDF data\n",
        "tfidf_normalized = normalize(tfidf, norm='l2')\n",
        "\n",
        "#Convert sparse matrix to dense array so can ues PCA\n",
        "X_normalizedde = tfidf_normalized.todense()\n",
        "\n",
        "#Retaining 95 % variance.\n",
        "pca = PCA(n_components = 0.95)\n",
        "tfidfn = pca.fit_transform(X_normalizedde)\n",
        "\n",
        "#Print differences\n",
        "print( \"Shape Of tfidf Before Normalise and PCA : \" , tfidf.shape)\n",
        "print( \"Shape Of tfidf After Normalise and PCA  : \" , tfidfn.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of tfidf Before Normalise and PCA :  (6000, 5174)\n",
            "Shape Of tfidf After Normalise and PCA  :  (6000, 2265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQaNFqWcPjXr",
        "colab_type": "text"
      },
      "source": [
        "## C3D PCA and Normalise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DLl_64ffG9b",
        "colab_type": "text"
      },
      "source": [
        "There are many benefits to dimensionality reduction. C3D was found to have 101 features. It would be possible that my training instances would not be spread out uniformly and therefore training any models on C3D could return biased results. Principal Component Analysis (PCA) is a widely used dimensionality reduction algorithm. I used the Scikit-Learns PCA class, setting a variance requirement of 95%. Using PCA I was able to reduce the dimensions down to 44, so that’s a 44% size reduction from its original size. This is a great improvement from the previous 101 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2LtMvaFRJg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8bad5db9-8c84-4408-d881-9c9ccf34f21b"
      },
      "source": [
        "#Normalise the data\n",
        "c3d_normalized = normalize(df_c3d, norm='l2')\n",
        "\n",
        "#Apply PCA\n",
        "pca = PCA(n_components = 0.95)\n",
        "c3dp = pca.fit_transform(c3d_normalized)\n",
        "\n",
        "#Print differences\n",
        "print( \"Shape Of C3D Before PCA : \" , df_c3d.shape)\n",
        "print( \"Shape Of C3D After PCA : \" , c3dp.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of C3D Before PCA :  (6000, 101)\n",
            "Shape Of C3D After PCA :  (6000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aZaG4ZxvBz9",
        "colab_type": "text"
      },
      "source": [
        "## HMP Normalise and PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrG7Ay46ayGy",
        "colab_type": "text"
      },
      "source": [
        "Similar to C3D, HMP requires normalisation before being applied to any model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKxbNwzrvFeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "87376a95-269b-4a9d-c2fb-211792e50205"
      },
      "source": [
        "#Normalise the data\n",
        "hmp_normalized = normalize(df_hmp, norm='l2')\n",
        "\n",
        "#Apply PCA\n",
        "pca = PCA(n_components = 0.95)\n",
        "hmpp = pca.fit_transform(hmp_normalized)\n",
        "\n",
        "#Print differences\n",
        "print( \"Shape Of C3D Before PCA : \" , df_hmp.shape)\n",
        "print( \"Shape Of C3D After PCA : \" , hmpp.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of C3D Before PCA :  (6000, 6075)\n",
            "Shape Of C3D After PCA :  (6000, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlJAOhkrGhZB",
        "colab_type": "text"
      },
      "source": [
        "## Combining Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0htqEl05A0TU",
        "colab_type": "text"
      },
      "source": [
        "### Combine Captions, HMP and C3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIh76kc5Qm5a",
        "colab_type": "text"
      },
      "source": [
        "Previous years work combined all features and then used them to train the model. To build on this I will combine some features, but I will also apply PCA dimensionality reduction which was not implemented. I will maintain 95% variance. I hope this will allow me to determine the best features while keeping dimensionality reduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uArdvAj1ztWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "78464fac-059d-4657-e3ef-7705c236976f"
      },
      "source": [
        "#Combining HMP, C3D and One hot encoded data into one numpy array\n",
        "cdd = np.concatenate((hmpp,c3dp,ohn), axis=1)\n",
        "\n",
        "#Apply PCA to the combination \n",
        "pca = PCA(n_components=0.95)\n",
        "ccdp = pca.fit_transform(cdd)\n",
        "\n",
        "#Print differences\n",
        "print(\"Before: \", cdd.shape)\n",
        "print(\"After :\", ccdp.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before:  (6000, 1852)\n",
            "After : (6000, 887)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR3ilnDuA3yk",
        "colab_type": "text"
      },
      "source": [
        "### Combine all text features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "001XG8FKMjpb",
        "colab_type": "text"
      },
      "source": [
        "During my research, I saw several studies which combined multiple pre-computed features together, for example like I have done in the previous cell. To the best of my knowledge it seemed that there was a lack of experiments in terms combining caption transformations. So here I am combining TF-IDF, One Hot Encoding and the Sequences. They all derive from the same feature so it is possible they will not return better results than each individually, however  I thought this was worth testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5HHMCyiA6He",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "38725f9c-8469-4c68-9e3b-74c4477a4b24"
      },
      "source": [
        "#Combining all text features\n",
        "allcaptions = np.concatenate((ohn,seq_normalized,tfidfn), axis=1)\n",
        "\n",
        "#Applying PCA to text features\n",
        "pca = PCA(n_components=0.95)\n",
        "allcaptionsp = pca.fit_transform(allcaptions)\n",
        "\n",
        "#Print differences\n",
        "print(\"Before: \", cdd.shape)\n",
        "print(\"After :\", ccdp.shape)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before:  (6000, 1852)\n",
            "After : (6000, 887)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I32IAzZ9TZLk",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMWTzNaMw8yW",
        "colab_type": "text"
      },
      "source": [
        "To test whether the data is linear or non-linear we used linear regression. Calculating R Squared of the predictions and the ground truth will allow us to identify this. As you can see the R squared result is negative which is a really bad score. This means that the data does not suit a linear model and therefore could be considered non-linear. This is really important to know going forward, for example, knowing the data is non-linear allows me to apply a kernelised SVR. As you can see from the results below every feature is non-linear expect one. It seems that TF-IDF could be considered linear and therefore we must take this into account when developing models later on down the line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V-iZuB0VOPU",
        "colab_type": "text"
      },
      "source": [
        "## Using Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZh6BsASVSAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "e137d044-f700-4ed5-c3b0-aa282e1324e3"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#The following code details the Short Term Score model\n",
        "#Setting my target vector\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "#Setting my feature matrix which is the normalised sequences.\n",
        "X = seq_normalized;\n",
        "#Splitting my data into training and validation data. Doing an 80% 20% split.\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Creating my Linear Regression model and setting it to run on all available threads so it will run faster.\n",
        "lr_c_s = LinearRegression(n_jobs=-1)\n",
        "#Training model on the feature matrix and target vector\n",
        "lr_c_s.fit(X_train_lr_st, Y_train_lr_st)\n",
        "#Using my trained model to predict the target vector based on some feature matrix and storing the results\n",
        "y_pred_rf_st = lr_c_s.predict(X_test_lr_st)\n",
        "#Calculating the Spearman coefficient score for my models predictions and the validation target vector\n",
        "Get_score(Y_test_lr_st, y_pred_rf_st)\n",
        "\n",
        "#The following code details the Long Term Score model\n",
        "#Setting up my feature matrix and target vector.\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X = seq_normalized;\n",
        "#Creating my training and validation data\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "lr_c_l = LinearRegression(n_jobs=-1)\n",
        "lr_c_l.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_rf_lt = lr_c_l.predict(X_test_lr_lt)\n",
        "#Calculating Sprearman score\n",
        "Get_score(Y_test_lr_lt, y_pred_rf_lt)\n",
        "\n",
        "# Test to whether the data is linear or non-linear\n",
        "print(r2_score(lr_c_s.predict(X_test_lr_st), Y_test_lr_st))\n",
        "print(r2_score(lr_c_l.predict(X_test_lr_lt), Y_test_lr_lt))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.058\n",
            "The Spearman's correlation coefficient is: 0.011\n",
            "-0.0008340283350369848\n",
            "-0.0008340412614020742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSRwEZg-jjXN",
        "colab_type": "text"
      },
      "source": [
        "## Using Captions(One-Hot Encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDQruGVNeVjS",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression model trained on the One Hot Encoded data which was normalised and dimensionality reduced. This model performs the best out of all Linear Regression models for both short- and long-term scores, with 0.319 for short-term and 0.122 for long-term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYIVDM47T16o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "00f94205-55f5-4e3e-805f-d94ab77820ed"
      },
      "source": [
        "#Short Term Score\n",
        "#Setting up features and target, splitting it into training and validation\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "#The feature matrix here is the One Hot Encoded data\n",
        "X = ohn;\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Building a new model which is to run tasks on all available cores\n",
        "lr_c_os = LinearRegression(n_jobs=-1)\n",
        "lr_c_os.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_rf_st = lr_c_os.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_rf_st)\n",
        "\n",
        "#Long Term Score\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X = ohn;\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_c_ol = LinearRegression(n_jobs=-1)\n",
        "lr_c_ol.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_rf_lt = lr_c_ol.predict(X_test_lr_lt)\n",
        "Get_score( Y_test_lr_lt, y_pred_rf_lt)\n",
        "\n",
        "\n",
        "# Test to whether the data is linear or non-linear\n",
        "print(r2_score(lr_c_os.predict(X_test_lr_st), Y_test_lr_st))\n",
        "print(r2_score(lr_c_ol.predict(X_test_lr_lt), Y_test_lr_lt))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.319\n",
            "The Spearman's correlation coefficient is: 0.122\n",
            "-5.3512749786932545e-14\n",
            "-3.774758283725532e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiw1M3guqrce",
        "colab_type": "text"
      },
      "source": [
        "## Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9VTCMFrLDe",
        "colab_type": "text"
      },
      "source": [
        "This Linear Regression model is training on the normalised TF-IDF data. This model does not perform as well as the one hot encoded data but performs better than the sequences. \n",
        "\n",
        "Previous to PCA being applied the spearman score was 0.225 for short term and 0.085 for long term. After PCA was applied these scores actually increased to 0.255 and  0.085.\n",
        "\n",
        "It is important to take note of the R squared results here. As we can see they results came back as positive integers compared to all other Linear Regression Models trained on the other features. It is possible that the TF-IDF data is linear. Whereas the rest of the features can be considered non-linear due to their negative scores. We would expect TF-IDF to perform better on Linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Um-eAAeqvoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "b982662f-055f-4648-acc5-a669549e8bd7"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Short Term Score\n",
        "#Setting up features and target, splitting it into training and validation\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "#The feature matrix is the normalised TF-IDF data\n",
        "X = tfidfn\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_tfs = LinearRegression(n_jobs=-1)\n",
        "lr_tfs.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_lrs = lr_tfs.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_lrs)\n",
        "\n",
        "#Long Term Score\n",
        "Y_l = df_labels[ 'long-term_memorability'].values\n",
        "#The feature matrix is the normalised TF-IDF data\n",
        "X = tfidfn\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_tfl = LinearRegression(n_jobs=-1)\n",
        "lr_tfl.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_lrl = lr_tfl.predict(X_test_lr_lt)\n",
        "Get_score(Y_test_lr_lt, y_pred_lrl)\n",
        " \n",
        "# Test to whether the data is linear or non-linear\n",
        "print(r2_score(y_pred_lrs, Y_test_lr_st))\n",
        "print(r2_score(y_pred_lrl, Y_test_lr_lt))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.257\n",
            "The Spearman's correlation coefficient is: 0.099\n",
            "5.6621374255882984e-14\n",
            "8.881784197001252e-16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtweP497twEc",
        "colab_type": "text"
      },
      "source": [
        "## Using C3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKprwP03dk4o",
        "colab_type": "text"
      },
      "source": [
        "Building a linear regression model for short- and long-term seperately. This model is trained on the C3D data which PCA was applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzgYxURxt0dW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "ca20c380-6327-4c22-919a-9125fbffb7ed"
      },
      "source": [
        "#Short-Term Memorability\n",
        "#Preparing data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = c3dp\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "lr_c = LinearRegression(n_jobs=-1)\n",
        "lr_c.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_lrs = lr_c.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_lrs)\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing data\n",
        "Y_l = df_labels[ 'long-term_memorability'].values\n",
        "X = c3dp\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "lr_l = LinearRegression(n_jobs=-1)\n",
        "lr_l.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_lrl = lr_l.predict(X_test_lr_lt)\n",
        "Get_score(Y_test_lr_lt, y_pred_lrl)\n",
        "\n",
        "\n",
        "# Test to whether the data is linear or non-linear\n",
        "print(r2_score(y_pred_lrs, Y_test_lr_st))\n",
        "print(r2_score(y_pred_lrl, Y_test_lr_lt))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.288\n",
            "The Spearman's correlation coefficient is: 0.116\n",
            "-7.410909888717088\n",
            "-34.46574790420082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbZnn4MFtzE4",
        "colab_type": "text"
      },
      "source": [
        "## Using HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDttoPlQd6A7",
        "colab_type": "text"
      },
      "source": [
        "Linear regression model is trained on the HMP dataset. PCA was applied to this data. The HMP data does not perform well as a feature matrix making this model the second worst out of all Linear Regression models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lTmyn4WvA98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "4e81d49a-4961-490b-ea11-f5b7ee4ff2fa"
      },
      "source": [
        "#Short-Term Memorability\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = hmpp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_c = LinearRegression(n_jobs=-1)\n",
        "lr_c.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = lr_c.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "#Long-Term Memorability\n",
        "Y_l = df_labels[ 'long-term_memorability'].values\n",
        "X = hmpp\n",
        "X_train_lh, X_test_lh, Y_train_lh, Y_test_lh = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_l = LinearRegression(n_jobs=-1)\n",
        "lr_l.fit(X_train_lh, Y_train_lh)\n",
        "y_pred_l = lr_l.predict(X_test_lh)\n",
        "Get_score(Y_test_lh, y_pred_l)\n",
        "\n",
        "\n",
        "\n",
        "# Test to whether the data is linear or non-linear\n",
        "print(r2_score(y_pred_s, Y_test_s))\n",
        "print(r2_score(y_pred_l, Y_test_lh))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.250\n",
            "The Spearman's correlation coefficient is: 0.114\n",
            "-10.923953658632914\n",
            "-43.37442228855988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dOOVZ3iXduQ",
        "colab_type": "text"
      },
      "source": [
        "##  Using Combination Of Captions, C3D and HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sFDeoWvkgi0",
        "colab_type": "text"
      },
      "source": [
        "This model outperforms all other linear regression models in both short and long term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Xm-mzkUNrR",
        "colab_type": "code",
        "outputId": "608dac80-6dd0-4a73-9278-7336b75820a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Short-Term\n",
        "#Preparing data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = ccdp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Model\n",
        "lr_cddps = LinearRegression(n_jobs=-1)\n",
        "lr_cddps.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = lr_cddps.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "#Long-Term\n",
        "#Preparing data\n",
        "Y_s = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Model\n",
        "lr_cddpl = LinearRegression(n_jobs=-1)\n",
        "lr_cddpl.fit(X_train_l, Y_train_l)\n",
        "Y_pred_l = lr_cddpl.predict(X_test_l)\n",
        "Get_score(Y_test_l, Y_pred_l)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.415\n",
            "The Spearman's correlation coefficient is: 0.172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkE6XS892SHB",
        "colab_type": "text"
      },
      "source": [
        "## Using Combination of Caption Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTiIGgpQ2pbo",
        "colab_type": "text"
      },
      "source": [
        "This model trained on all the text transformations does not perform as well as the comination model above. However, it performs better than each of the individual text models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaKFBbn_2UP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6ed39333-52d1-446a-f561-90996aa92b34"
      },
      "source": [
        "#Short-Term\n",
        "#Preparing data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = allcaptionsp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Model\n",
        "lr_alltexts = LinearRegression(n_jobs=-1)\n",
        "lr_alltexts.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = lr_alltexts.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "#Long-Term\n",
        "#Preparing data\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Model\n",
        "lr_alltextsl = LinearRegression(n_jobs=-1)\n",
        "lr_alltextsl.fit(X_train_l, Y_train_l)\n",
        "Y_pred_l = lr_alltextsl.predict(X_test_l)\n",
        "Get_score(Y_test_l, Y_pred_l)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.362\n",
            "The Spearman's correlation coefficient is: 0.150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW6Pa50mTdt_",
        "colab_type": "text"
      },
      "source": [
        "# Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kVgsoojf_oT",
        "colab_type": "text"
      },
      "source": [
        "Moving on from Linear regression I implemented Ridge regression which is a regularised version of Linear Regression. This approach will allow me to control the weights of the data. As this is a regularized model it is important to ensure the data has been normalised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJsn7UdrSeGK",
        "colab_type": "text"
      },
      "source": [
        "## Using Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjR1fuLQSgrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fef79278-34f1-45e4-cc07-b1c4139dfdde"
      },
      "source": [
        "#Short-Term Prediction\n",
        "#Preparing the data\n",
        "Y = df_labels['short-term_memorability'].values\n",
        "X = seq_normalized;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Ridge model wiht aplha 2\n",
        "r_c_s = Ridge(alpha = 2.0)\n",
        "r_c_s.fit(X_train_s, Y_train_s)\n",
        "Y_pred_s = r_c_s.predict(X_test_s)\n",
        "Get_score(Y_test_s, Y_pred_s)\n",
        "\n",
        "#Long-Term Prediction\n",
        "#Preparing the data\n",
        "Y = df_labels['long-term_memorability'].values\n",
        "X = seq_normalized;\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Ridge model wiht aplha 2\n",
        "r_c_l = Ridge(alpha = 2.0)\n",
        "r_c_l.fit(X_train_l, Y_train_l)\n",
        "Y_pred_l = r_c_l.predict(X_test_l)\n",
        "Get_score(Y_test_l, Y_pred_l)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.058\n",
            "The Spearman's correlation coefficient is: 0.015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niAQYZpElEzH",
        "colab_type": "text"
      },
      "source": [
        "## Using Captions (One-Hot Encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiG2pZd6vomR",
        "colab_type": "text"
      },
      "source": [
        "This Ridge regression model is trained on normalised One Hot Encoded data. It scored very high with 0.445 and 0.166. The model performed poorer on the original data with scores of: 0.405 and 0.169 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43o3jXeykhB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7490e1fd-15cf-430c-f2d0-87210e58da0d"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "#Short-Term\n",
        "Y = df_labels['short-term_memorability'].values\n",
        "X = ohn;\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "r_c_s = Ridge(alpha = 2.0)\n",
        "r_c_s.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_rf_st = r_c_s.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_rf_st)\n",
        "\n",
        "\n",
        "#Long-Term\n",
        "Y = df_labels['long-term_memorability'].values\n",
        "X = ohn;\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "r_c_l = Ridge(alpha = 2.0)\n",
        "r_c_l.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_r_lt = r_c_l.predict(X_test_lr_lt)\n",
        "Get_score(Y_test_lr_lt, y_pred_r_lt)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.445\n",
            "The Spearman's correlation coefficient is: 0.169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMqV2mQs7Gnr",
        "colab_type": "text"
      },
      "source": [
        "## Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IZ4OnSttio4",
        "colab_type": "text"
      },
      "source": [
        "We can see that using TF-IDF for the long term predictions performs much better than using the one hot encoded data or the sequences. However there is very little difference for the short term score with 0.001 of a difference. Normalizing the data did not make much of a different in terms of performance. Before normalising the scores were 0.445 and 0.192."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS6StVt87J-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4e785ab1-63c8-43a1-99e3-cadccaf5bdfa"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Short term\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = tfidfn\n",
        "X_train_r_st, X_test_r_st, Y_train_r_st, Y_test_r_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Increasing the alpha \n",
        "r_tfs = Ridge(alpha = 3.0)\n",
        "r_tfs.fit(X_train_r_st, Y_train_r_st)\n",
        "y_pred_r_st = r_tfs.predict(X_test_r_st)\n",
        "Get_score(Y_test_r_st, y_pred_r_st)\n",
        "\n",
        "#Long term\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_r_lt, X_test_r_lt, Y_train_r_lt, Y_test_r_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Increasing the alpha \n",
        "r_tfl = Ridge(alpha = 3.0)\n",
        "r_tfl.fit(X_train_r_lt, Y_train_r_lt)\n",
        "y_pred_r_lt = r_tfl.predict(X_test_r_lt)\n",
        "Get_score(Y_test_r_lt, y_pred_r_lt)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.446\n",
            "The Spearman's correlation coefficient is: 0.191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvmuecX6xhbD",
        "colab_type": "text"
      },
      "source": [
        "## Using C3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AvTP2FWxlIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c286876-ced8-4c35-f3a9-5fba8b6dff58"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Short-Term\n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = c3dp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_c = Ridge(alpha = 3.0)\n",
        "r_c.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = r_c.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_l = Ridge(alpha = 3.0)\n",
        "r_l.fit(X_train_l, Y_train_l)\n",
        "y_pred_l = r_l.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_l)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.287\n",
            "The Spearman's correlation coefficient is: 0.118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53OwDd4xjo0",
        "colab_type": "text"
      },
      "source": [
        "## Using HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYexzNjXyBRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0be8a853-19e2-45a0-a11f-d956fb0455eb"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Short-Term \n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = hmpp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_h = Ridge(alpha = 3.0)\n",
        "r_h.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = r_h.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X = hmpp;\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_hl = Ridge(alpha = 3.0)\n",
        "r_hl.fit(X_train_l, Y_train_l)\n",
        "y_pred_ll = r_hl.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_ll)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.253\n",
            "The Spearman's correlation coefficient is: 0.114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob3p5XDHknaZ",
        "colab_type": "text"
      },
      "source": [
        "##  Using Combination Of Captions, C3D and HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPd2fDU6lT4p",
        "colab_type": "text"
      },
      "source": [
        "This model is trained the combination of captions, C3D and HMP. This model performs the best for the long term memorability score. It outperforms all other models trained on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nurvbUr6kssZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b58bacbe-767f-46d3-c24a-65679fc0c225"
      },
      "source": [
        "#Short-Term \n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = ccdp\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_ccd_s = Ridge(alpha = 2.0)\n",
        "r_ccd_s.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_rf_st = r_ccd_s.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_rf_st)\n",
        "\n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X = ccdp\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_ccd_l = Ridge(alpha = 2.0)\n",
        "r_ccd_l.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_rf_st = r_ccd_l.predict(X_test_lr_lt)\n",
        "Get_score(Y_test_lr_lt, y_pred_rf_st)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.443\n",
            "The Spearman's correlation coefficient is: 0.176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT4LCdTQ263l",
        "colab_type": "text"
      },
      "source": [
        "## Using Combination Of Caption Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ-wVLJp3Zd-",
        "colab_type": "text"
      },
      "source": [
        "This combination does not perform as well here compared to the individual transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdui95rg2_mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0cead988-8c66-4be3-c6bb-9bb7df7d51ab"
      },
      "source": [
        "#Short-Term \n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values\n",
        "X = allcaptionsp\n",
        "X_train_lr_st, X_test_lr_st, Y_train_lr_st, Y_test_lr_st = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_alltexts = Ridge(alpha = 2.0)\n",
        "r_alltexts.fit(X_train_lr_st, Y_train_lr_st)\n",
        "y_pred_alls = r_alltexts.predict(X_test_lr_st)\n",
        "Get_score(Y_test_lr_st, y_pred_alls)\n",
        "\n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X_train_lr_lt, X_test_lr_lt, Y_train_lr_lt, Y_test_lr_lt = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "r_alltextsl = Ridge(alpha = 2.0)\n",
        "r_alltextsl.fit(X_train_lr_lt, Y_train_lr_lt)\n",
        "y_pred_alll = r_alltextsl.predict(X_test_lr_lt)\n",
        "Get_score(Y_test_lr_lt, y_pred_alll)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.436\n",
            "The Spearman's correlation coefficient is: 0.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5h71shKzAw7",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR4kcCgH_fDC",
        "colab_type": "text"
      },
      "source": [
        "Multiple decision tree regression models were implemented. Unfortunately, as you will see from the results, these models performed very poorly. In fact, they performed the worst out of all models. One solution to a model underperforming is to move onto a more complicated model. So, later on in the notebook you will see that I implemented Random Forest which yields much better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdBkWvdbu5X7",
        "colab_type": "text"
      },
      "source": [
        "##  Using Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVcJfQbgzNIn",
        "colab_type": "text"
      },
      "source": [
        "For the decision tree model, the end nodes are our target vectors, the memorability scores. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aanLJyXfwCp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "99d0f38d-12fb-46a8-c619-ee46f04d5139"
      },
      "source": [
        "#Short-Term \n",
        "Y_s = df_labels['short-term_memorability'].values \n",
        "X = seq_normalized \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Creating Decision tree model\n",
        "dt_s = DecisionTreeRegressor(random_state = 0,  max_depth=17)  \n",
        "dt_s.fit(X_train, Y_train) \n",
        "Y_pred_s = dt_s.predict(X_test) \n",
        "Get_score(Y_test, Y_pred_s) \n",
        "\n",
        "#Long-Term\n",
        "Y_l = df_labels['long-term_memorability'].values \n",
        "X = seq_normalized \n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#Creating Decision tree model\n",
        "dt_l = DecisionTreeRegressor(random_state = 0,  max_depth=17)  \n",
        "dt_l.fit(X_train_l, Y_train_l) \n",
        "Y_pred_l = dt_l.predict(X_test_l) \n",
        "Get_score(Y_test_l, Y_pred_l) \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.117\n",
            "The Spearman's correlation coefficient is: 0.066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnVEyslJUB0O",
        "colab_type": "text"
      },
      "source": [
        "## Using Captions(One-hot Encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTkYdxbTWaty",
        "colab_type": "text"
      },
      "source": [
        "If deicison trees are not restricted in terms of tree depth when they are growing, they will more than likely overfit the data. It is very important to test different tree depths out, choosing one that will result in a tree not too complex. \n",
        "\n",
        "I wanted to test the performance of the model on normalised and non normalised data. Applying this model to the one hot encoded data that was normalised but did not have PCA applied to it resulted in really good Spearman scores. Short term got 0.295 and long term got 0.044.\n",
        "\n",
        "When the model is applied to the normalised data the scores are 0.167\n",
        "and 0.058 respectivly.\n",
        "\n",
        "Here letting the tree grow unrestricted actually resulted in a lower Sprearman score. This is probably due to the fact that the tree would fit too closely to the trianing data and then when it was predicting based on data it had not seen before it could not predict as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpLYq77HYruB",
        "colab_type": "text"
      },
      "source": [
        "#### Using Original Data( Not normalised or pca applied)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmzLhTW4Yvue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "869f50d3-0d3d-4466-9cbf-e80bf69b6173"
      },
      "source": [
        "#Short-Term \n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values \n",
        "X = one_hot_res \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "  \n",
        "#Implementing restricted decision tree.\n",
        "dt_oh = DecisionTreeRegressor(random_state = 0, max_depth=17)  \n",
        "dt_oh.fit(X_train, Y_train) \n",
        "Y_pred_s = dt_oh.predict(X_test) \n",
        "Get_score(Y_test, Y_pred_s) \n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values \n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "  \n",
        "#Implementing restricted decision tree.\n",
        "dt_oh = DecisionTreeRegressor(random_state = 0,max_depth=17)  \n",
        "dt_oh.fit(X_train_l, Y_train_l) \n",
        "Y_pred_l = dt_oh.predict(X_test_l) \n",
        "Get_score(Y_test_l, Y_pred_l) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.293\n",
            "The Spearman's correlation coefficient is: 0.070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27qg_kCgYoIf",
        "colab_type": "text"
      },
      "source": [
        "### Using Normalised Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC6YgfBEVdRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "587f34c1-a211-4b0f-ab13-ad743bb25e5c"
      },
      "source": [
        "#Short-Term \n",
        "#Preparing the data\n",
        "Y_s = df_labels['short-term_memorability'].values \n",
        "X = ohn\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_s, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing restricted decision tree.\n",
        "dt_oh = DecisionTreeRegressor(random_state = 0, max_depth=17)  \n",
        "dt_oh.fit(X_train, Y_train) \n",
        "Y_pred_s = dt_oh.predict(X_test) \n",
        "Get_score(Y_test, Y_pred_s) \n",
        "\n",
        "#Long-Term\n",
        "#Preparing the data\n",
        "Y_l = df_labels['long-term_memorability'].values \n",
        "X = ohn\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing restricted decision tree.\n",
        "dt_oh = DecisionTreeRegressor(random_state = 0,max_depth=17)  \n",
        "dt_oh.fit(X_train_l, Y_train_l) \n",
        "Y_pred_l = dt_oh.predict(X_test_l) \n",
        "Get_score(Y_test_l, Y_pred_l) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.167\n",
            "The Spearman's correlation coefficient is: 0.058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlcSsaOT7j6Z",
        "colab_type": "text"
      },
      "source": [
        "## Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHTT_RFctv8D",
        "colab_type": "text"
      },
      "source": [
        "The Spearman's correlation coefficient is: 0.231\n",
        "The Spearman's correlation coefficient is: 0.048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOa7XBf37n7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2551d055-0093-4ed3-ed32-eccd9b849851"
      },
      "source": [
        "#Short Term\n",
        "Y = df_labels['short-term_memorability'].values \n",
        "X = tfidfn\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "dt_t = DecisionTreeRegressor(random_state = 0, max_depth=17)  \n",
        "dt_t.fit(X_train_s, Y_train_s) \n",
        "Y_pred_s = dt_t.predict(X_test_s) \n",
        "Get_score(Y_test_s, Y_pred_s) \n",
        "\n",
        "#Long Term\n",
        "Y = df_labels['long-term_memorability'].values \n",
        "X = tfidfn\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "dt_t = DecisionTreeRegressor(random_state = 0, max_depth=17)  \n",
        "dt_t.fit(X_train_l, Y_train_l) \n",
        "Y_pred_l = dt_t.predict(X_test_l) \n",
        "Get_score(Y_test_l, Y_pred_l) "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.174\n",
            "The Spearman's correlation coefficient is: 0.011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Kgc_FFPHv1",
        "colab_type": "text"
      },
      "source": [
        "## Using C3D "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsCOGN6SKh-",
        "colab_type": "text"
      },
      "source": [
        "C3D has consistently performed poorly so the low scores for short and long term are expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgiCA0xqPMOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7ecb3b33-334d-402c-dd37-b60cd0196bce"
      },
      "source": [
        "Y = df_labels[['short-term_memorability','long-term_memorability']].values \n",
        "X = c3dp \n",
        "X_train_dt_st, X_test_dt_st, Y_train_dt_st, Y_test_dt_st = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "  \n",
        "dt_cd = DecisionTreeRegressor(random_state = 42, max_depth=20)  \n",
        "dt_cd.fit(X_train_dt_st, Y_train_dt_st) \n",
        "Y_pred_dt_oh = dt_cd.predict(X_test_dt_st) \n",
        "Get_score(Y_test_dt_st, Y_pred_dt_oh) \n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.075\n",
            "The Spearman's correlation coefficient is: 0.033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36S5nRS0HbF",
        "colab_type": "text"
      },
      "source": [
        "## Using HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UtvZsds0I8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e069b175-dc27-430c-e739-4c56902e2d77"
      },
      "source": [
        "Y = df_labels[['short-term_memorability','long-term_memorability']].values \n",
        "X = hmpp \n",
        "X_train_dt_st, X_test_dt_st, Y_train_dt_st, Y_test_dt_st = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "  \n",
        "dt_cd = DecisionTreeRegressor(random_state = 42, max_depth=20)  \n",
        "dt_cd.fit(X_train_dt_st, Y_train_dt_st) \n",
        "Y_pred_dt_oh = dt_cd.predict(X_test_dt_st) \n",
        "Get_score(Y_test_dt_st, Y_pred_dt_oh) \n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.085\n",
            "The Spearman's correlation coefficient is: 0.019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo2k3hfioDKj",
        "colab_type": "text"
      },
      "source": [
        "##  Using Combination Of Captions, C3D and HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldn43GB8oGIh",
        "colab_type": "text"
      },
      "source": [
        "This decision tree is trained on the combination of Captions, C3D and HMP. Not restricting the tree depth results in a lower Spearman score, due to decision trees overfitting issue.\n",
        "4 was found to be the optimum tree depth. Higher depths resulted in a lower Spearman score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ie2oY5YoH53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "44fcbf67-b19c-4cc8-9c69-9ff716dcfba1"
      },
      "source": [
        "#Short and long term \n",
        "#Preparing the data\n",
        "Y = df_labels[['short-term_memorability','long-term_memorability']].values \n",
        "X = ccdp \n",
        "X_train_sl, X_test_sl, Y_train_sl, Y_test_sl = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "dt_cda = DecisionTreeRegressor(random_state = 42, max_depth=4)  \n",
        "dt_cda.fit(X_train_sl, Y_train_sl) \n",
        "Y_pred_dt_a = dt_cda.predict(X_test_sl) \n",
        "Get_score(Y_test_sl, Y_pred_dt_a) \n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.270\n",
            "The Spearman's correlation coefficient is: 0.107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZx8e1Y3ghw",
        "colab_type": "text"
      },
      "source": [
        "## Using Combination of Caption Tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4BVixMI4N2n",
        "colab_type": "text"
      },
      "source": [
        "This model doesnt perform overly well for short-term scores. It is the second highest scorer of decision trees in terms of long-term scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kShLJO3kNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a0eac6bf-b8dc-47da-c3c5-bb35c5ca526c"
      },
      "source": [
        "#Short and long term \n",
        "#Preparing the data\n",
        "Y = df_labels[['short-term_memorability','long-term_memorability']].values \n",
        "X = allcaptionsp \n",
        "X_train_sl, X_test_sl, Y_train_sl, Y_test_sl = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing the model\n",
        "df_alltext = DecisionTreeRegressor(random_state = 42, max_depth=3)  \n",
        "df_alltext.fit(X_train_sl, Y_train_sl) \n",
        "Y_pred_alls = df_alltext.predict(X_test_sl) \n",
        "Get_score(Y_test_sl, Y_pred_alls) \n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.210\n",
            "The Spearman's correlation coefficient is: 0.090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD2uSl7eWuhd",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2co4bXdCjC3",
        "colab_type": "text"
      },
      "source": [
        "To improve the best performing SVR model I wanted to change the sampling method. I wanted to test whether bootstrapping over randomly sampling would improve my SVR model. Below you will see the subheadings for the model with bootstrapping and without bootstrapping. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RexUFI2F4bm",
        "colab_type": "text"
      },
      "source": [
        "## Using Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaWaeQRWF6ET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d8e4f327-2f7b-4061-be05-3b54cde7992c"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = seq_normalized;\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X , Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "svr_st_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3 , epsilon=0.1, gamma='scale', kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_st_c.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_st_c.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st,y_pred_svr_st )\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X , Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "svr_lt_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_lt_c.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_lt_c.predict(X_test_svr_lt)\n",
        "Get_score(y_pred_svr_lt, Y_test_svr_lt )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.070\n",
            "The Spearman's correlation coefficient is: 0.062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z1pSd6zmQga",
        "colab_type": "text"
      },
      "source": [
        "## Using Captions (One Hot Encoding )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoWyFFCFVQ15",
        "colab_type": "text"
      },
      "source": [
        "Bootstraping was tested on the Support Vector Machine for the one hot encoded data. It was found that the model did performed  better with bootstraping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4akxVEIsTj",
        "colab_type": "text"
      },
      "source": [
        "### Without Bootstraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho8Bp6Q36jMy",
        "colab_type": "text"
      },
      "source": [
        "This model was tested on both normalised and the raw one hot encoded data. The model performs better on the normalised data with a short-term score of 0.416 and long-term of 0.180. The scores on the original data was 0.376 and 0.161 respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btBiZYrl6h2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a5ba53d4-e0f4-4f55-831b-f730c000512d"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "#X_oh = one_hot_res;\n",
        "X_oh = ohn\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "svr_st = SVR(kernel=\"rbf\", degree=2, C=100, epsilon=0.1)\n",
        "svr_st.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_st.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st)\n",
        "\n",
        "\n",
        "#long term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "svr_lt = SVR(kernel=\"rbf\", degree=2, C=100, epsilon=0.1)\n",
        "svr_lt.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_lt.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_svr_lt)\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.416\n",
            "The Spearman's correlation coefficient is: 0.180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyvvZcHVIxhQ",
        "colab_type": "text"
      },
      "source": [
        "### With bootstraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMRiiQKJfuN",
        "colab_type": "text"
      },
      "source": [
        "SVM performs better with bootstrapping enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1XGQs6Iz6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f2fdb521-aefc-411c-9f8b-de5d93d356f1"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "#X_oh = one_hot_res;\n",
        "X_oh = ohn\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "svr_st = SVR(kernel=\"rbf\", degree=2, C=100, epsilon=0.1)\n",
        "n_estimators = 2\n",
        "svm_bag = BaggingRegressor(svr_st, n_estimators=n_estimators, bootstrap=True)\n",
        "svm_bag.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svm_bag.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st)\n",
        "\n",
        "#long term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "svr_lt = SVR(kernel=\"rbf\", degree=2, C=100, epsilon=0.1)\n",
        "n_estimators = 2\n",
        "svm_bag_l = BaggingRegressor(svr_lt, n_estimators=n_estimators, bootstrap=True)\n",
        "svm_bag_l.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svm_bag_l.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_svr_st)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.413\n",
            "The Spearman's correlation coefficient is: 0.170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NLHTsi5T_Ke",
        "colab_type": "text"
      },
      "source": [
        "## Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsuZw0JEzoee",
        "colab_type": "text"
      },
      "source": [
        "SVR with Tf-IDF is one of the best performing models so far. In an attempt to improve the model even further I used Grid Search to find the optimum hyperparameter values. The short-term score was 0.436 and the long-term score was 0.160 before tuning. After the tuned parameters were used the scores were"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcMPMOue5Y5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4c738085-0e92-4b7b-a8fa-456d3b5e13db"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing Model\n",
        "svr_st_t = SVR(kernel=\"poly\", degree=3, C=100, epsilon=0.1)\n",
        "svr_st_t.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_st_t.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st)\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X, Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing Model\n",
        "svr_lt_t = SVR(kernel=\"poly\", degree=3, C=100, epsilon=0.1)\n",
        "svr_lt_t.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_lt_t.predict(X_test_svr_lt)\n",
        "Get_score(y_pred_svr_lt, Y_test_svr_lt )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.436\n",
            "The Spearman's correlation coefficient is: 0.160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS2G_Zj1sv2R",
        "colab_type": "text"
      },
      "source": [
        "### Optimise with Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gIIWe0A3_ja",
        "colab_type": "text"
      },
      "source": [
        "I am using grid search to find the optimum combinations of the hyperparameters available for SVR. Because I am mostly concerned with Spearman score in this study I have created my own scorer. This will be used by Grid Search CV to evaluate the predictions on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9BlaN5sux8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a2780f8b-d7ab-4f5c-f4ff-acff3213a3a4"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "#Defining my Spearman score function. This one is slightly different than the one seen previously.\n",
        "def cv_Spearman(Y_true, Y_pred):\n",
        "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
        "    Y_pred = np.squeeze(Y_pred)\n",
        "    Y_true = np.squeeze(Y_true)\n",
        "    if Y_pred.shape != Y_true.shape:\n",
        "        print('Input shapes don\\'t match!')\n",
        "    else:\n",
        "        if len(Y_pred.shape) == 1:\n",
        "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
        "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
        "            #Return the score only, no text because valid scorer must only return single value.\n",
        "            return score_mat.iloc[1][0]\n",
        "        else:\n",
        "            for ii in range(Y_pred.shape[1]):\n",
        "                cv_Spearman(Y_pred[:,ii],Y_true[:,ii])\n",
        "\n",
        "#Making my Spearman function an actual scorer than can be used.\n",
        "my_scorer = make_scorer(cv_Spearman, greater_is_better=True)\n",
        "\n",
        "#Find best hyperparameters for short term SVR model which was already trained above\n",
        "parameterss = {'kernel':('linear', 'rbf', 'poly'),'degree':[1, 2, 3] ,'C':[1, 10, 100], 'epsilon':[1.0, 0.1, 0.5]}\n",
        "gridmodels = svr_st_t\n",
        "gridsearch_svr = GridSearchCV(gridmodels, parameterss, scoring=my_scorer, n_jobs=-1)\n",
        "gridsearch_svr.fit(X_train_svr_st, Y_train_svr_st)\n",
        "\n",
        "\n",
        "#Find best hyperparameters for long-term SVR model which was already trained above\n",
        "parametersl = {'kernel':('linear', 'rbf', 'poly'),'degree':[1, 2, 3] ,'C':[1, 10, 100], 'epsilon':[1.0, 0.1, 0.5]}\n",
        "gridmodell = svr_lt_t\n",
        "gridsearch_svrl = GridSearchCV(gridmodell, parametersl, scoring=my_scorer, n_jobs=-1)\n",
        "gridsearch_svrl.fit(X_train_svr_st, Y_train_svr_st)\n",
        "\n",
        "print(\"Short-Term Best Params: \", gridsearch_svr.best_params_)\n",
        "print(\"Long-Term Best Params: \", gridsearch_svrl.best_params_)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Short-Term Best Params:  {'C': 1, 'degree': 2, 'epsilon': 0.1, 'kernel': 'poly'}\n",
            "Long-Term Best Params:  {'C': 1, 'degree': 2, 'epsilon': 0.1, 'kernel': 'poly'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8siOQ9y5yPZ0",
        "colab_type": "text"
      },
      "source": [
        "### Retrain model using best hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pIuUKBv5aaj",
        "colab_type": "text"
      },
      "source": [
        "Now that I have the best hyperparameters values for my models I will change these and retrain the model. Because I defined my own scorer to be Spearman score the scores should improve somewhat. \n",
        "\n",
        "As you can see from the results of the previous cell, the optimum combination of hyperparameters were:\n",
        "\n",
        "{'C': 1, 'degree': 2, 'epsilon': 0.1, 'kernel': 'poly'}\n",
        "\n",
        "Unfortunetly this does not actually improve the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDaY_5rgyTzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "024b12c4-9fed-4f31-9ff2-aacf051383b8"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "svr_s_optim = SVR(kernel=\"poly\", degree=2, C=1, epsilon=0.1)\n",
        "svr_s_optim.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_s_optim.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X, Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing model\n",
        "svr_l_optim = SVR(kernel=\"poly\", degree=2, C=1, epsilon=0.1)\n",
        "svr_l_optim.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_l_optim.predict(X_test_svr_lt)\n",
        "Get_score(y_pred_svr_lt, Y_test_svr_lt )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.430\n",
            "The Spearman's correlation coefficient is: 0.151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGK4Z-1cgRc",
        "colab_type": "text"
      },
      "source": [
        "## Using C3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_XsMvIMkQnL",
        "colab_type": "text"
      },
      "source": [
        "Applying the pca reduced C3D features to my SVR model improved the Spearman score, however it is still not at a high level.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mypmVoUcjX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1f38a8ee-e9be-4bdf-fea5-2b99f42066b7"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_c = c3dp;\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X_c, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "#svr_st_c3d = SVR(kernel=\"rbf\", degree=3, C=200, epsilon=0.1)\n",
        "svr_st_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3 , epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_st_c.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_st_c.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st )\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X_c, Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing model\n",
        "#svr_lt_c3d = SVR(kernel=\"rbf\", degree=3, C=200, epsilon=0.1)\n",
        "svr_lt_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_lt_c.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_lt_c.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_svr_lt )\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.247\n",
            "The Spearman's correlation coefficient is: 0.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT_46U6JHb3X",
        "colab_type": "text"
      },
      "source": [
        "## Using HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "123mTlO2HeMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "443f6075-4226-4bb6-e3c9-fc93f3a3e8b3"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = hmpp;\n",
        "X_train_svr_st, X_test_svr_st, Y_train_svr_st, Y_test_svr_st = train_test_split(X , Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "#svr_st_c3d = SVR(kernel=\"rbf\", degree=3, C=200, epsilon=0.1)\n",
        "svr_st_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3 , epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_st_c.fit(X_train_svr_st, Y_train_svr_st)\n",
        "y_pred_svr_st = svr_st_c.predict(X_test_svr_st)\n",
        "Get_score(Y_test_svr_st, y_pred_svr_st)\n",
        "\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X , Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing model\n",
        "#svr_lt_c3d = SVR(kernel=\"rbf\", degree=3, C=200, epsilon=0.1)\n",
        "svr_lt_c = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_lt_c.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_lt_c.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_svr_lt )\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.242\n",
            "The Spearman's correlation coefficient is: 0.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPmD9RI6oc3X",
        "colab_type": "text"
      },
      "source": [
        "##  Using Combination Of Captions, C3D and HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAOtp8gQoejc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b452b541-d791-44eb-8e9c-ce44f8c09df2"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = ccdp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X , Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "svr_as = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3 , epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_as.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = svr_as.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X , Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing model\n",
        "svr_al = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_al.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_svr_lt = svr_al.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_svr_lt )\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.396\n",
            "The Spearman's correlation coefficient is: 0.151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3mgMwC4e2Y",
        "colab_type": "text"
      },
      "source": [
        "## Using Combination of Caption Tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxslybuk5UoH",
        "colab_type": "text"
      },
      "source": [
        "This model has mediocre performance. So far the combination of caption transformations has not been a top feature for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zExRWE84gbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "41e48658-ac6f-4a75-902b-de99b7c07d84"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = allcaptionsp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X , Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "svr_alltexts = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3 , epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_alltexts.fit(X_train_s, Y_train_s)\n",
        "y_pred_alls = svr_alltexts.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_alls)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_svr_lt, X_test_svr_lt, Y_train_svr_lt, Y_test_svr_lt = train_test_split(X , Y_lt, test_size=0.2, random_state=109)\n",
        "\n",
        "#Implementing model\n",
        "svr_alltextsl = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "svr_alltextsl.fit(X_train_svr_lt, Y_train_svr_lt)\n",
        "y_pred_alll= svr_alltextsl.predict(X_test_svr_lt)\n",
        "Get_score(Y_test_svr_lt, y_pred_alll )\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.404\n",
            "The Spearman's correlation coefficient is: 0.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skq6UHrpIilr",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tHlycikAIVP",
        "colab_type": "text"
      },
      "source": [
        "As discussed previously, the individual decision trees did not perform very well. Therefore, I decided to implement Random Forest models. They are simply an ensemble of decision trees. Ensemble approaches can often be found to perform much better than the individual algorithms. Thankfully, this was the case here and the random forest models  yield much more promising spearman scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "askwD0hUSRxT",
        "colab_type": "text"
      },
      "source": [
        "## Using Sequence of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRUZt8TGSVgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b05e9f43-65a0-4fa3-8652-3f77b008a8d3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Preparing Data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = seq_normalized;\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "rf_st_s = RandomForestRegressor(n_estimators= 15, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 40, bootstrap= False)\n",
        "rf_st_s.fit(X_train_rf_st, Y_train_rf_st)\n",
        "y_pred_rf_st = rf_st_s.predict(X_test_rf_st)\n",
        "Get_score(y_pred_rf_st, Y_test_rf_st)\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_rf_lt, X_test_rf_lt, Y_train_rf_lt, Y_test_rf_lt = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implementing model\n",
        "rf_lt_s = RandomForestRegressor(n_estimators= 15, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 40, bootstrap= True)\n",
        "rf_lt_s.fit(X_train_rf_lt, Y_train_rf_lt)\n",
        "y_pred_rf_st = rf_lt_s.predict(X_test_rf_lt)\n",
        "Get_score(y_pred_rf_st, Y_test_rf_lt)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.232\n",
            "The Spearman's correlation coefficient is: 0.069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnYdfK_sJAs4",
        "colab_type": "text"
      },
      "source": [
        "## Using Captions (One Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b18tFOmxpiq",
        "colab_type": "text"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-LYsXMvGeKj",
        "colab_type": "text"
      },
      "source": [
        "Random forest model for one hot encoding was tested with three feature matrices. The original one hot encoded data, the normalised data and then the noramlised data which was reduced with PCA. Random forest performed best on the original data and worse on the PCA data.\n",
        "\n",
        "I have used Random seach to zone in on a range of values for the RF hyperparameters. Using these new parameters increased performance by 0.01 which is not great.\n",
        "\n",
        "Enabling bootsraping instead of randomly sampling the data improves the Spearman score, with 0.31 before and 0.44 after. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REQbHCWX64Fi",
        "colab_type": "text"
      },
      "source": [
        "#### Using Normalised Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xyfet2a_RkQ",
        "colab_type": "text"
      },
      "source": [
        "I found the model performed better with bootstrapping disabled. The spearman score declined when enabled. This model is one of the top performers we have seen so far \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppkHYMW4Iorb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "059a9ac3-a335-4f57-8235-6dcc84ccbb68"
      },
      "source": [
        "##short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_oh = oh_normalized\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_st_c = RandomForestRegressor(n_estimators= 100, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 90, bootstrap= False)\n",
        "rf_st_c.fit(X_train_rf_st, Y_train_rf_st)\n",
        "y_pred_rf_st = rf_st_c.predict(X_test_rf_st)\n",
        "Get_score(Y_test_rf_st,y_pred_rf_st)\n",
        "\n",
        "\n",
        "#Long-Term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_rf_lt, X_test_rf_lt, Y_train_rf_lt, Y_test_rf_lt = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_lt_c = RandomForestRegressor(n_estimators= 100, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 80, bootstrap= False)\n",
        "rf_lt_c.fit(X_train_rf_lt, Y_train_rf_lt)\n",
        "y_pred_rf_lt = rf_lt_c.predict(X_test_rf_lt)\n",
        "Get_score(Y_test_rf_lt, y_pred_rf_lt)\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.445\n",
            "The Spearman's correlation coefficient is: 0.135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp7komQA6-74",
        "colab_type": "text"
      },
      "source": [
        "#### Using Normalised and PCA reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6F8Ky94mSNm",
        "colab_type": "text"
      },
      "source": [
        "Here I wanted to test if the random forest model would perform better with less input features. The normalised PCA reduced data has (6000, 1778) features whereas the normalised data has (6000, 5191) features. Even though the smaller data maintained a 95% variance after dimensionality reduction we see a big decrease in the spreaman score. The model trained on the normalised data performed much better with a short term-score of X and a long-term score of Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Smukh67BwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2a56fd9a-fdb5-42d4-bdb3-f055ae888e2e"
      },
      "source": [
        "##short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_oh = ohn\n",
        "\n",
        "#impf = rf_st_c.feature_importances_\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "#dont change cos gives better results.\n",
        "rf_st_cn = RandomForestRegressor(n_estimators= 25, min_samples_split= 5, min_samples_leaf= 4, max_features= 'sqrt', max_depth=15, bootstrap= True)\n",
        "rf_st_cn.fit(X_train_rf_st, Y_train_rf_st)\n",
        "y_pred_rf_st = rf_st_cn.predict(X_test_rf_st)\n",
        "Get_score(Y_test_rf_st,y_pred_rf_st)\n",
        "\n",
        "\n",
        "# #long term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_rf_lt, X_test_rf_lt, Y_train_rf_lt, Y_test_rf_lt = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_st_cn = RandomForestRegressor(n_estimators= 25, min_samples_split= 5, min_samples_leaf= 4, max_features= 'sqrt', max_depth=15, bootstrap= True)\n",
        "rf_st_cn.fit(X_train_rf_lt, Y_train_rf_lt)\n",
        "y_pred_rf_lt = rf_st_cn.predict(X_test_rf_lt)\n",
        "Get_score(Y_test_rf_lt, y_pred_rf_lt)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.306\n",
            "The Spearman's correlation coefficient is: 0.112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRTQkEQ5YN1J",
        "colab_type": "text"
      },
      "source": [
        "#### Use Random Grid Search to Speed Up Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JRPVZeZhQ9",
        "colab_type": "text"
      },
      "source": [
        "Because the random forest model with the One-Hot encoded data has performed as one of the best models so far we can use Grid Search to find a combination of  hyperparameters which might improve the model further. We find the range for values short and long term predictions. This range will narrow down the possible values for the RF parameters. We expect to see an improvement when using the range. We test a number of different hyperparameters, the number of trees, maximum features, tree depth and so on. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqc0u0_LYTJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6e79d6d2-7997-460a-a4e3-70d8f8f33ed1"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "#Making my Spearman function an actual scorer than can be used.\n",
        "my_scorer = make_scorer(cv_Spearman, greater_is_better=True)\n",
        "\n",
        "##short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_oh = oh_normalized\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#Find best hyperparameters for short term RF model which was already trained above\n",
        "parameterss_randf = {'n_estimators':[5, 10, 15, 20 ,25],'max_depth':[2, 5, 10, 20, 30]}\n",
        "model_to_improve_short = rf_st_c\n",
        "gridsearch_rfs = GridSearchCV(model_to_improve_short, parameterss_randf, scoring=my_scorer, n_jobs=-1)\n",
        "gridsearch_rfs.fit(X_train_rf_st, Y_train_rf_st)\n",
        "\n",
        "#Find best hyperparameters for long-term RF model which was already trained above\n",
        "model_to_improve_long = rf_lt_c\n",
        "gridsearch_rfsl = GridSearchCV(model_to_improve_long, parameterss_randf, scoring=my_scorer, n_jobs=-1)\n",
        "gridsearch_rfsl.fit(X_train_svr_st, Y_train_svr_st)\n",
        "\n",
        "print(\"Short-Term Best Params: \", gridsearch_rfs.best_params_)\n",
        "print(\"Long-Term Best Params: \", gridsearch_rfsl.best_params_)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Short-Term Best Params:  {'max_depth': 30, 'n_estimators': 20}\n",
            "Long-Term Best Params:  {'max_depth': 10, 'n_estimators': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6PERvdDz1vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "24e6a979-6c3d-4f9d-9653-fae18b42a4d9"
      },
      "source": [
        "##short term memorability\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_oh = oh_normalized\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_st_c_tuned = RandomForestRegressor(n_estimators= 20, max_depth= 30)\n",
        "rf_st_c_tuned.fit(X_train_rf_st, Y_train_rf_st)\n",
        "y_pred_rf_st_tuned = rf_st_c_tuned.predict(X_test_rf_st)\n",
        "Get_score(Y_test_rf_st,y_pred_rf_st_tuned)\n",
        "\n",
        "\n",
        "#Long-Term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_rf_lt, X_test_rf_lt, Y_train_rf_lt, Y_test_rf_lt = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_lt_c_tuned = RandomForestRegressor(n_estimators= 25, max_depth= 10)\n",
        "rf_lt_c_tuned.fit(X_train_rf_lt, Y_train_rf_lt)\n",
        "y_pred_rf_lt_tuned = rf_lt_c_tuned.predict(X_test_rf_lt)\n",
        "Get_score(Y_test_rf_lt, y_pred_rf_lt_tuned)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.360\n",
            "The Spearman's correlation coefficient is: 0.108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMqZZ28I75nL",
        "colab_type": "text"
      },
      "source": [
        "## Using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFGiYlwlUFnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "185b6d4a-c9b0-4842-cf97-97c3a8e38ba9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Prepare data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_rf_st, X_test_rf_st, Y_train_rf_st, Y_test_rf_st = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement model\n",
        "rf_t_s = RandomForestRegressor(n_estimators= 100, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 90, bootstrap= False)\n",
        "#rf_t_s = RandomForestRegressor(n_estimators=100, max_depth=100, n_jobs=-1)\n",
        "rf_t_s.fit(X_train_rf_st, Y_train_rf_st)\n",
        "y_pred_rf_st = rf_t_s.predict(X_test_rf_st)\n",
        "Get_score(Y_test_rf_st, y_pred_rf_st)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Prepare data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_rf_lt, X_test_rf_lt, Y_train_rf_lt, Y_test_rf_lt = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement model\n",
        "rf_t_l = RandomForestRegressor(n_estimators= 100, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 90, bootstrap= False)\n",
        "rf_t_l.fit(X_train_rf_lt, Y_train_rf_lt)\n",
        "y_pred_rf_l = rf_t_l.predict(X_test_rf_lt)\n",
        "Get_score(Y_test_rf_lt, y_pred_rf_l)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.336\n",
            "The Spearman's correlation coefficient is: 0.153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR-RMVQsUJUT",
        "colab_type": "text"
      },
      "source": [
        "## Using C3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qogiB9TeUMUi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ca0bb9ed-92b2-46c7-e4fd-fbe23bc6eaaf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Prepare data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_c3d = c3dp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X_c3d, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement model\n",
        "rf_cds = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "rf_cds.fit(X_train_s, Y_train_s)\n",
        "y_pred_rf_cds = rf_cds.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_rf_cds)\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Prepare data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X_c3d, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement model\n",
        "rfcdl = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "rfcdl.fit(X_train_l, Y_train_l)\n",
        "y_pred_l= rfcdl.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_l)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.235\n",
            "The Spearman's correlation coefficient is: 0.084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHCiyCJ_EPEp",
        "colab_type": "text"
      },
      "source": [
        "## Using HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcz-pqtfEVW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1d9064c0-69b8-4d4d-938b-9d7c768d67d8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Prepare data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = hmpp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement Random Forest\n",
        "random_forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "random_forest.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = random_forest.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Prepare data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement Random Forest\n",
        "random_forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "random_forest.fit(X_train_l, Y_train_l)\n",
        "y_pred_l = random_forest.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_l)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.271\n",
            "The Spearman's correlation coefficient is: 0.075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyn8KcK3ow03",
        "colab_type": "text"
      },
      "source": [
        "## Using Combination Of Captions, C3D and HMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-11k6LHo0I2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4aa76189-1814-4cbd-a05c-0c7b73571835"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Prepare data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = ccdp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement RF Model\n",
        "rfas = RandomForestRegressor( n_jobs=-1, max_depth = 50)\n",
        "rfas.fit(X_train_s, Y_train_s)\n",
        "y_pred_s = rfas.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_s)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Prepare data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement RF Model\n",
        "rfal = RandomForestRegressor(n_jobs=-1, max_depth = 50)\n",
        "rfal.fit(X_train_l, Y_train_l)\n",
        "y_pred_l = rfal.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_l)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.387\n",
            "The Spearman's correlation coefficient is: 0.189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpCvIdLr5eNM",
        "colab_type": "text"
      },
      "source": [
        "## Using Combinatino of Caption Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q0KPNm55hsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "69a8e56f-4138-417f-9509-1f5340f81fc3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Memorability\n",
        "#Prepare data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = allcaptionsp;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement RF Model\n",
        "rf_textalls = RandomForestRegressor( n_jobs=-1, max_depth = 50)\n",
        "rf_textalls.fit(X_train_s, Y_train_s)\n",
        "y_pred_sall = rf_textalls.predict(X_test_s)\n",
        "\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Prepare data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement RF Model\n",
        "rf_textall = RandomForestRegressor(n_jobs=-1, max_depth = 50)\n",
        "rf_textall.fit(X_train_l, Y_train_l)\n",
        "y_pred_lall = rf_textall.predict(X_test_l)\n",
        "\n",
        "Get_score(Y_test_s, y_pred_sall)\n",
        "Get_score(Y_test_l, y_pred_lall)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.357\n",
            "The Spearman's correlation coefficient is: 0.169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJCejXGMdu2u",
        "colab_type": "text"
      },
      "source": [
        "#Ensemble Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sag1n7DUd5V-",
        "colab_type": "text"
      },
      "source": [
        "##Voting Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3z8cpI7JK0T",
        "colab_type": "text"
      },
      "source": [
        "For this ensemble method we will combine all the best performing models. The feature matrix will be the feature that has performed the best to this point. That is the one hot encoded captions. It should be the case that the voting regressor performs better than the rest of the models.\n",
        "\n",
        "This model will not be tested on all features. It will only be tested on the best performing ones. Those were TF-IDF, One Hot Encoding and the combination of captions, C3D and HMP.\n",
        "\n",
        "Decision trees are not tested here as random forest is an ensemble of decision trees. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QbzL0z2_jEf",
        "colab_type": "text"
      },
      "source": [
        "### Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4dKY2gBS9Hm",
        "colab_type": "text"
      },
      "source": [
        "I tested all combinations of models to see which resulted in the optimum score. The results of each combination is below. The best combination for both long- and short-term was SVR and Ridge.\n",
        "\n",
        "The voting regressor performs better than any of the estimators so far. This was expected as ensemble methods usually perform better than their base estimators. The score is not improved by much however so I want to emply another ensemble method.\n",
        "\n",
        "\n",
        "**Model Combinations Short-Term Test Results** \n",
        "\n",
        "Linear, Ridge, SVR, Random Forest(RF) = 0.344\n",
        "\n",
        "Linear, Ridge, SVR =0.328\n",
        "\n",
        "Linear, Ridge =  0.316\n",
        "\n",
        "Linear, SVR = 0.279\n",
        "\n",
        "Linear, RF = 0.297\n",
        "\n",
        "**Ridge, SVR =  0.455**\n",
        "\n",
        "SVR, RF = 0.443\n",
        "\n",
        "\n",
        "**Model Combinations Long-Term Test Results** \n",
        "\n",
        "Linear, Ridge, SVR, Random Forest(RF) = 0.143\n",
        "\n",
        "Ridge, SVR, RF = 0.194\n",
        "\n",
        "Linear, Ridge, SVR = 0.136\n",
        "\n",
        "Linear, Ridge =  0.122\n",
        "\n",
        "Linear, SVR = 0.120\n",
        "\n",
        "Linear, RF = 0.116\n",
        "\n",
        "**Ridge, SVR =  0.201**\n",
        "\n",
        "SVR, RF = 0.184\n",
        "\n",
        "Ridge, RF =  0.196\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L4G_s9D_nQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f50e6002-a1d9-4a32-9d3b-f79cdcdff990"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Preparing the Short-Term test data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn\n",
        "X_train_oh, X_test_oh, Y_train_oh, Y_test_oh = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Defining the all  the Short Term Prediction Models so we can choose which ones to implement here\n",
        "#linears = lr_tfs #Linear Regression\n",
        "ridges = r_tfs # Ridge Regression\n",
        "svrs = svr_st_t # Support Vector Machine\n",
        "#rfs = rf_t_s # Random Forest\n",
        "\n",
        "#Defining our models that the voting regressor will actually use\n",
        "estimators = [('ridges', ridges), ('svrs', svrs)]\n",
        "\n",
        "#Implementing voting regressor with our estimators defined above. Set to run in parallel. \n",
        "voting_s = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
        "#Voting being trained on TFIDF normalised data\n",
        "voting_s.fit(X_train_oh, Y_train_oh)\n",
        "y_pred_v = voting_s.predict(X_test_oh)\n",
        "Get_score(Y_test_oh, y_pred_v)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preparing the Long-Term Data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X = tfidfn\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#Defining all the Long-Term Prediction Models we have created so far.\n",
        "#linearltf = lr_tfl #Linear Regression\n",
        "ridgeltf = r_tfl  # Ridge Regression\n",
        "svrltf = svr_lt_t  #Support Vector Regression\n",
        "#randfortf = rf_t_l #Random Forest\n",
        "          \n",
        "#Defining our estimators that the voting regressor will use to get votes.\n",
        "estimators = [('ridgeltf', ridgeltf) , ('svrltf', svrltf)]\n",
        "\n",
        "#Creating voting regressor\n",
        "voting_long_tf = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
        "voting_long_tf.fit(X_train_l, Y_train_l)\n",
        "y_pred_vtf = voting_long_tf.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_vtf)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.455\n",
            "The Spearman's correlation coefficient is: 0.201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BX3lTd5kMqc",
        "colab_type": "text"
      },
      "source": [
        "### Using Captions(one-hot encoded) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BloAd8d_L2R",
        "colab_type": "text"
      },
      "source": [
        "The voting regressor trained on the one hot encoded data does not perform as well as the model above trained on TF-IDF data. It has a lower short term score with 0.002 difference which is very small. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3UoeYmeYgQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ef399080-4a9b-4b4f-9a81-e8c5cf74e390"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Short-Term Score\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_oh = ohn\n",
        "X_train_oh, X_test_oh, Y_train_oh, Y_test_oh = train_test_split(X_oh, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Implement Voting Regressor\n",
        "votingtso = VotingRegressor(estimators=[ ('r_c_s', r_c_s), ('svr_st', svr_st)])\n",
        "votingtso.fit(X_train_oh, Y_train_oh)\n",
        "y_pred_vso = votingtso.predict(X_test_oh)\n",
        "Get_score(Y_test_oh, y_pred_vso)\n",
        "\n",
        "#Long-Term\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X_oh, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#The estimators here are the best performing ones.\n",
        "votingtlo = VotingRegressor( estimators=[('r_c_l', r_c_l), ('svr_lt', svr_lt)])\n",
        "votingtlo.fit(X_train_oh, Y_train_oh)\n",
        "y_pred_vloh = votingtlo.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_vloh)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.445\n",
            "The Spearman's correlation coefficient is: 0.192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKZGo9p4Qbgi",
        "colab_type": "text"
      },
      "source": [
        "### Using Combination of Captions, C3d and HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JBerp-D_hEP",
        "colab_type": "text"
      },
      "source": [
        "This model performs worst out of the voting models so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHDRKfgPQl7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c46c6673-4e61-4b43-afb9-c51a589f2a21"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#short term test split\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = ccdp\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Short Term Prediction Models\n",
        "ridgesa = r_ccd_s\n",
        "svrsa = svr_as\n",
        "\n",
        "estimators = [ ('ridgesa', ridgesa), ('svrsa', svrsa) ]\n",
        "    \n",
        "#short term\n",
        "voting_sa = VotingRegressor(estimators=estimators)\n",
        "voting_sa.fit(X_train_s, Y_train_s)\n",
        "y_pred_va = voting_sa.predict(X_test_s)\n",
        "Get_score(Y_test_s, y_pred_va)\n",
        "\n",
        "#Long Term Score\n",
        "Y_l = df_labels['long-term_memorability'].values\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l  = train_test_split(X, Y_l, test_size=0.2, random_state=42)\n",
        "\n",
        "#Long Term Prediction Models\n",
        "ridgesal = r_ccd_l\n",
        "svrsal = svr_al\n",
        "\n",
        "estimators = [ ('ridgesal', ridgesal), ('svrsal', svrsal) ]\n",
        "\n",
        "#short term\n",
        "voting_lac = VotingRegressor(estimators=estimators)\n",
        "voting_lac.fit(X_train_l, Y_train_l)\n",
        "y_pred_vl = voting_lac.predict(X_test_l)\n",
        "Get_score(Y_test_l, y_pred_vl)\n",
        "\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.441\n",
            "The Spearman's correlation coefficient is: 0.171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ehbodYsnwca",
        "colab_type": "text"
      },
      "source": [
        "## Stacking Ensemble Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmeQbtCEobef",
        "colab_type": "text"
      },
      "source": [
        "This is the best model for short-term predictions. \n",
        "\n",
        "Using the predictions for the same algorithms used in the ensemble method as input for my blender. We used a hold-out set to train the blender. The goal here is to improve the learning process. The incorrect predictions are valued in this model and are used to improve performance. The model learns from mistakes. Each tier will learn from the mistakes of the tier before it.\n",
        "\n",
        "For the stacking ensemble this will only be tested with the TF-IDF data as this has consistently performed the best.\n",
        "\n",
        "The final estimator for this model will be the Ridge regression model as this performed very well so far.\n",
        "\n",
        "The stacking regrossor provides the ability to determine the cross-validation splitting. Here I have set this to use 5-fold cross validation. This is a K-fold split.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg1BvY697K2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "124f787f-7006-47ea-fa48-d108db3631ac"
      },
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "#Short Term Prediction Models\n",
        "#The final estimator is going to be Ridge model because it performed best out of all others so far.\n",
        "ridges = r_tfs # Ridge Regression\n",
        "svrs = svr_st_t # Support Vector Machine\n",
        "final_estimator = ridges \n",
        "\n",
        "#short term test split\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Defining Models To Use\n",
        "estimators = [ ('svrs', svrs), ('voting_s', voting_s)]\n",
        "\n",
        "#With hyperparameters setting the estimators and final estimator for regressor to use\n",
        "stackeds = StackingRegressor(estimators=estimators, final_estimator=final_estimator, cv=5)\n",
        "stackeds.fit(X_train_s, Y_train_s)\n",
        "y_pred_stacked = stackeds.predict(X_test_s)\n",
        "Get_score( Y_test_s, y_pred_stacked)\n",
        "\n",
        "\n",
        "#Long Term Prediction Models\n",
        "#The final estimator is going to be Ridge model because it performed best out of all others so far.\n",
        "final_estimator_long = r_tfl \n",
        "svrsal = svr_lt_t \n",
        "\n",
        "#Long term test split\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l  = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#Defining Models To Use\n",
        "estimators_long = [ ('svrsal', svrsal), ('voting_long_tf', voting_long_tf)]\n",
        "\n",
        "#With hyperparameters setting the estimators and final estimator for regressor to use\n",
        "stackedl = StackingRegressor(estimators=estimators_long, final_estimator=final_estimator_long)\n",
        "stackedl.fit(X_train_l, Y_train_l)\n",
        "y_pred_stackedl = stackedl.predict(X_test_l)\n",
        "Get_score( Y_test_l, y_pred_stackedl)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.457\n",
            "The Spearman's correlation coefficient is: 0.199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htZpx0awBCif",
        "colab_type": "text"
      },
      "source": [
        "## Bagging Ensemble Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tj7xwZo__qm",
        "colab_type": "text"
      },
      "source": [
        "Final approach is to implement bagging ensemble. The short-term score is high but not as high as the stacking regressor model. However, the long-term score is the highest score we have seen so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csVTxu32BFi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "02915a13-7243-41ce-f8a0-1998ff4127f3"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "#Short-term memorability score\n",
        "#Preparing data\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#Defining base estimator for bagging regressor. Using best estimator we have so far.\n",
        "best_est_short = stackeds\n",
        "\n",
        "#Implementing model\n",
        "bag_reg = BaggingRegressor(base_estimator = best_est_short, n_estimators=10, random_state=0)\n",
        "bag_reg.fit(X_train_s, Y_train_s)\n",
        "preds = bag_reg.predict(X_test_s)\n",
        "Get_score(Y_test_s, preds)\n",
        "\n",
        "\n",
        "#Long-Term Memorability\n",
        "#Preparing data\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X = tfidfn;\n",
        "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#Define base estimator for bagging regressor to use. Using best estimator we have so far.\n",
        "base_est_long =  stackedl\n",
        "\n",
        "#Implementing model\n",
        "bag_reg_long = BaggingRegressor(base_estimator = base_est_long, n_estimators=10, random_state=0)\n",
        "bag_reg_long.fit(X_train_l, Y_train_l)\n",
        "preds_long = bag_reg_long.predict(X_test_l)\n",
        "Get_score(Y_test_l, preds_long)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Spearman's correlation coefficient is: 0.454\n",
            "The Spearman's correlation coefficient is: 0.210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCD8tUunyK3",
        "colab_type": "text"
      },
      "source": [
        "## Cross Validation on best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0asVPm6zATj",
        "colab_type": "text"
      },
      "source": [
        "I apply 2-fold cross validation to my model. This is a really important step because it allows me to get a really good idea as to the performance of my best model. The data is split into 2 groups and each group it used to evaluate the model. I have set the scoring of the cross validation to the Spearman Coefficient score. This test will allow me to see the mean Spearman score after a 2 fold validation test. \n",
        "\n",
        "The results below are not as high as the original score, however it proves that this model is better performing than nearly all other models implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOnJ2S7Vn09z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f67b4aa9-ecb2-4249-d4c2-d00ee6e9d232"
      },
      "source": [
        "#12:24\n",
        "f:rom sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def cv_Spearman(Y_true, Y_pred):\n",
        "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
        "    Y_pred = np.squeeze(Y_pred)\n",
        "    Y_true = np.squeeze(Y_true)\n",
        "    if Y_pred.shape != Y_true.shape:\n",
        "        print('Input shapes don\\'t match!')\n",
        "    else:\n",
        "        if len(Y_pred.shape) == 1:\n",
        "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
        "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
        "            return score_mat.iloc[1][0]\n",
        "        else:\n",
        "            for ii in range(Y_pred.shape[1]):\n",
        "                cv_Spearman(Y_pred[:,ii],Y_true[:,ii])\n",
        "\n",
        "\n",
        "#short term memorability\n",
        "X = tfidfn\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_st, test_size=0.2, random_state=42)\n",
        "\n",
        "#This is my best performing model for short term with a score of 0.457\n",
        "model = stackeds \n",
        "\n",
        "my_scorer = make_scorer(cv_Spearman, greater_is_better=True)\n",
        "scores = cross_val_score(model, X, Y_st, scoring=my_scorer, cv=2)\n",
        "tree_sp_scores = np.mean(scores)\n",
        "\n",
        "\n",
        "#long term memorability\n",
        "X = tfidfn\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_lt, test_size=0.2, random_state=42)\n",
        "\n",
        "#This is my best performing model for long term with a score of 0.201\n",
        "model_l = bag_reg_long \n",
        "\n",
        "scores_long = cross_val_score(model_l, X, Y_lt, scoring=my_scorer, cv=2)\n",
        "tree_sp_scores_long = np.mean(scores_long)\n",
        "\n",
        "\n",
        "print(\"Short term: \", tree_sp_scores)\n",
        "print(\"Long term: \", tree_sp_scores_long)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Short term:  0.40352935567133413\n",
            "Long term:  0.16308589010330357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19-xHvw4A44A",
        "colab_type": "text"
      },
      "source": [
        "# Train Final Best Models On Full Dev-set Data And Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7-QHz00qcf9",
        "colab_type": "text"
      },
      "source": [
        "The optimum model for short and long term perdictions will be used here to predict the memorability scores for the test data. The best model for short-term scores was the Stacking Regressor with 0.457. The best model for long-term was the Bagging regressor with 0.210.\n",
        "\n",
        "We will now train these models on the full 6000 instances. Then we will use the test set of captions to predict their memorability.  We were not provided with the ground truth for this data and therefore cannot calcualte a Spearman score. I am simply saving the results in a template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkqW8zoEyOtf",
        "colab_type": "text"
      },
      "source": [
        "## Import Test-Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ5ANYh3AzpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functions to load captions\n",
        "def read_caps(fname):\n",
        "    \"\"\"Load the captions into a dataframe\"\"\"\n",
        "    vn = []\n",
        "    cap = []\n",
        "    df = pd.DataFrame();\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            pairs = line.split()\n",
        "            vn.append(pairs[0])\n",
        "            cap.append(pairs[1])\n",
        "        df['video']=vn\n",
        "        df['caption']=cap\n",
        "    return df\n",
        "\n",
        "\n",
        "# Load in the test set captions which we will try predict the memorability score for\n",
        "cap_path_tmp_test = './Test-set/Captions_test/test-set-1_video-captions.txt'\n",
        "df_cap_tmp_test = read_caps(cap_path_tmp_test)\n",
        "\n",
        "# Load in a template to store the predictions\n",
        "test_ground_truth = pd.read_csv('./Test-set/Ground-truth_test/ground_truth_template.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdVRxGGeFXmP",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Test Captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmUdLpHdFbHj",
        "colab_type": "text"
      },
      "source": [
        "As I found TF-IDF to work the best with the best performing models I will apply TF-IDF to the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bYpcmIdsH3F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "76a77d74-63fd-4dda-fbae-094e8d180c72"
      },
      "source": [
        "#Setup our Counter object which can assist with cleaning the captions\n",
        "vocab_test = Counter()\n",
        "\n",
        "#Loop through each caption and clean\n",
        "for i, capitalLetter in enumerate(df_cap_tmp_test['caption']):\n",
        "    # Removing dashes in between words and convert words to lowercase.\n",
        "    text = ''.join([c if c not in punctuation else ' ' for c in capitalLetter]).lower()\n",
        "    #At each row of iteration i save the updated text\n",
        "    df_cap_tmp_test.loc[i,'caption'] = text\n",
        "    vocab_test.update(text.split())\n",
        "\n",
        "\n",
        "caps_test = list(df_cap_tmp_test.caption.values)\n",
        "\n",
        "#Applying the pipeline I made earlier which applies CountVectorizer and TFIDF\n",
        "tfidf_test = tfidf_pipe.transform(caps_test)\n",
        "\n",
        "#Normalising TFIDF data\n",
        "tfidf_normalized_test = normalize(tfidf_test, norm='l2')\n",
        "\n",
        "#Convert sparse matrix to dense array so can ues PCA\n",
        "X_normalizedde_test = tfidf_normalized_test.todense()\n",
        "\n",
        "#Retaining 95 % variance.\n",
        "pca_test = PCA(n_components = 0.95)\n",
        "tfidfn_test = pca.transform(X_normalizedde_test)\n",
        "\n",
        "#We need to make sure there are the same number of features for training and testing\n",
        "print( \"Shape Of TEST TF-IDF After Normalise and PCA  : \" , tfidfn_test.shape)\n",
        "print(\" Shape of DEV TF-IDF training data used earlier : \", tfidfn.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Of TEST TF-IDF After Normalise and PCA  :  (2000, 2265)\n",
            " Shape of DEV TF-IDF training data used earlier :  (6000, 2265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOYn4GiWA9bM",
        "colab_type": "text"
      },
      "source": [
        "# Final Predictions Using Test-Set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Q6ZFMIA_Ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "def8ee08-52de-471d-9259-3cc3607fe1e1"
      },
      "source": [
        "#Short term\n",
        "#We need to train the best model on the whole 6000 instances, no validation check now.\n",
        "Y_st = df_labels['short-term_memorability'].values\n",
        "X = tfidfn\n",
        "\n",
        "#We need to predict the test data but we dont have the ground truth so not training split.\n",
        "X_test = tfidfn_test\n",
        "\n",
        "# Final Short-term prediction mode\n",
        "#Stacking Model from earlier\n",
        "final_short = stackeds \n",
        "#train final model on all 6000 instances\n",
        "final_short.fit(X, Y_st) \n",
        "#Save predictions so we can convert to csv\n",
        "y_pred_short = final_short.predict(X_test)\n",
        "print(X_test.shape, X.shape)\n",
        "\n",
        "\n",
        "#Final Long-term predictions\n",
        "Y_lt = df_labels['long-term_memorability'].values\n",
        "X = tfidfn\n",
        "X_test = tfidfn_test\n",
        "\n",
        "# Best long-term memorability model was the Bagging model \n",
        "final_long = bag_reg_long\n",
        "final_long.fit(X, Y_lt)\n",
        "y_pred_long = final_long.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 2265) (6000, 2265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8mZLHCGujiy",
        "colab_type": "text"
      },
      "source": [
        "##  Save predications as CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEN_Ajjculk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ground_truth['short-term_memorability'] = y_pred_short\n",
        "test_ground_truth['long-term_memorability'] = y_pred_long\n",
        "test_ground_truth.to_csv('/content/drive/My Drive/Assignment/Dataset/Maureen_Maguire_19213997_predictions.csv',  index=False)\n",
        "\n",
        "test_ground_truth.tail()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14-n-yzO1VII",
        "colab_type": "text"
      },
      "source": [
        "# Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ1lBKpp1Ydf",
        "colab_type": "text"
      },
      "source": [
        "Spearman: https://drive.google.com/drive/folders/1puG9lLjao1y4ZngKHJFpxi4Yl-9cHvV7\n",
        "\n",
        "Sequences and One-hot Encoding:\n",
        "https://drive.google.com/drive/folders/1puG9lLjao1y4ZngKHJFpxi4Yl-9cHvV7\n",
        "\n",
        "SVM Bootstraping:\n",
        "https://www.researchgate.net/figure/A-general-architecture-of-bootstrapping-using-a-single-SVM-model_fig2_275238559\n",
        "https://stats.stackexchange.com/questions/183230/bootstrapping-confidence-interval-from-a-regression-prediction\n",
        "\n",
        "Stacking Ensemble Method:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html\n",
        "\n",
        "Learning Curve:\n",
        "https://github.com/mainkoon81/DCU_project-01-Competition-MediaEval/blob/master/MinKun_ML_Script.ipynb"
      ]
    }
  ]
}